{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from tqdm import tqdm as _tqdm\n",
    "\n",
    "import random\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "def tqdm(*args, **kwargs):\n",
    "    return _tqdm(*args, **kwargs, mininterval=1)  # Safety, do not overflow buffer\n",
    "\n",
    "EPS = float(np.finfo(np.float32).eps)\n",
    "\n",
    "assert sys.version_info[:3] >= (3, 6, 0), \"Make sure you have Python 3.6 installed!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniconda/base/envs/rl2019/lib/python3.7/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.envs.make(\"CartPole-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# # The nice thing about the CARTPOLE is that it has very nice rendering functionality (if you are on a local environment). Let's have a look at an episode\n",
    "# obs = env.reset()\n",
    "# env.render()\n",
    "# done = False\n",
    "# while not done:\n",
    "#     obs, reward, done, _ = env.step(env.action_space.sample())\n",
    "#     env.render()\n",
    "#     time.sleep(0.05)\n",
    "# env.close()  # Close the environment or you will have a lot of render screens soon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_s, num_a, num_hidden=128):\n",
    "        nn.Module.__init__(self)\n",
    "        self.l1 = nn.Linear(num_s, num_hidden)\n",
    "        self.l2 = nn.Linear(num_hidden, num_a)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # YOUR CODE HERE\n",
    "        x = F.relu(self.l1(x))\n",
    "        x = self.l2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's instantiate and test if it works\n",
    "num_hidden = 128\n",
    "torch.manual_seed(1234)\n",
    "model = QNetwork(num_hidden=num_hidden, num_a=2, num_s=4)\n",
    "\n",
    "torch.manual_seed(1234)\n",
    "test_model = nn.Sequential(\n",
    "    nn.Linear(4, num_hidden), \n",
    "    nn.ReLU(), \n",
    "    nn.Linear(num_hidden, 2)\n",
    ")\n",
    "\n",
    "x = torch.rand(10, 4)\n",
    "\n",
    "# If you do not need backpropagation, wrap the computation in the torch.no_grad() context\n",
    "# This saves time and memory, and PyTorch complaints when converting to numpy\n",
    "with torch.no_grad():\n",
    "    assert np.allclose(model(x).numpy(), test_model(x).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    \n",
    "    def __init__(self, capacity, useTrick=True):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        # useTrick defines whether or not experience replay is really used\n",
    "        self.useTrick = useTrick\n",
    "\n",
    "    def push(self, transition):\n",
    "        # YOUR CODE HERE\n",
    "        if len(self.memory) >= self.capacity:\n",
    "            # if memory is full we remove the last value and add the new value at beginning\n",
    "            # use a deque next time\n",
    "            self.memory.insert(0, transition)\n",
    "            self.memory = self.memory[:-1]\n",
    "        else:\n",
    "            self.memory.insert(0, transition)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \n",
    "        if self.useTrick:\n",
    "            # random batch\n",
    "            return random.sample(self.memory, batch_size)\n",
    "        else:\n",
    "            # latest batch_size memories\n",
    "            return self.memory[-batch_size:]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    \n",
    "memory = ReplayMemory(2)\n",
    "memory.push(2)\n",
    "memory.push(3)\n",
    "memory.push(4)\n",
    "assert memory.memory[0] == 4\n",
    "assert memory.memory[1] == 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_epsilon(it):\n",
    "    return max(1 - 0.00095*it, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(model, state, epsilon):\n",
    "    # YOUR CODE HERE\n",
    "    _rand = random.random()\n",
    "    state = torch.from_numpy(state).float()\n",
    "    if _rand < epsilon:\n",
    "        \n",
    "        #TODO this is now hardcoded to return either 0 or 1, but might be different for different envs.\n",
    "        \n",
    "        # random move left (0) or right (1)\n",
    "        return torch.randint(2, (1,)).item()\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            return torch.argmax(model(state)).item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "s = env.reset()\n",
    "a = select_action(model, s, 0.05)\n",
    "assert not torch.is_tensor(a)\n",
    "print (a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_q_val(model, state, action):\n",
    "    # YOUR CODE HERE\n",
    "    # https://stackoverflow.com/questions/50999977/what-does-the-gather-function-do-in-pytorch-in-layman-terms\n",
    "    # model(state) contains estimated q values from the model\n",
    "    # action contains the action chosen by the model at that point\n",
    "    return model(state).gather(1, action.unsqueeze(1))\n",
    "    \n",
    "    \n",
    "class targetComputer():\n",
    "    def __init__(self, target_network_steps=0):\n",
    "        self.UPDATE_STEPS = target_network_steps\n",
    "        self.target_network_steps = self.UPDATE_STEPS\n",
    "        \n",
    "    def compute_target(self, model, reward, next_state, done, discount_factor):\n",
    "        # done is a boolean (vector) that indicates if next_state is terminal (episode is done)\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        # only update self.model to given model is target_network_steps = 0\n",
    "        if not hasattr(self, 'model'):\n",
    "            self.model = model\n",
    "        elif self.target_network_steps == 0:\n",
    "            self.model = model\n",
    "            self.target_network_steps = self.UPDATE_STEPS\n",
    "        \n",
    "        max_Q_prime = torch.zeros(next_state.shape[0])  # batch size\n",
    "\n",
    "        # create mask of non-done states\n",
    "        non_final_mask = torch.tensor(tuple(map(lambda s: s != 1, done)), dtype=torch.bool)\n",
    "\n",
    "        # when state is not done we add discount_factor * max() else 0\n",
    "        max_Q_prime[non_final_mask] = torch.max(self.model(next_state[non_final_mask]), 1).values\n",
    "        \n",
    "        self.target_network_steps -= 1\n",
    "\n",
    "        return (reward + discount_factor * max_Q_prime).unsqueeze(1)\n",
    "\n",
    "def train(model, memory, optimizer, batch_size, discount_factor, TargetComputer):\n",
    "    # DO NOT MODIFY THIS FUNCTION\n",
    "    \n",
    "    # don't learn without some decent experience\n",
    "    if len(memory) < batch_size:\n",
    "        return None\n",
    "\n",
    "    # random transition batch is taken from experience replay memory\n",
    "    transitions = memory.sample(batch_size)\n",
    "    \n",
    "    \n",
    "    # transition is a list of 4-tuples, instead we want 4 vectors (as torch.Tensor's)\n",
    "    state, action, reward, next_state, done = zip(*transitions)\n",
    "    \n",
    "    # convert to PyTorch and define types\n",
    "    state = torch.tensor(state, dtype=torch.float)\n",
    "    action = torch.tensor(action, dtype=torch.int64)  # Need 64 bit to use them as index\n",
    "    next_state = torch.tensor(next_state, dtype=torch.float)\n",
    "    reward = torch.tensor(reward, dtype=torch.float)\n",
    "    done = torch.tensor(done, dtype=torch.uint8)  # Boolean\n",
    "    \n",
    "    # compute the q value\n",
    "    q_val = compute_q_val(model, state, action)\n",
    "    \n",
    "    with torch.no_grad():  # Don't compute gradient info for the target (semi-gradient)\n",
    "        target = TargetComputer.compute_target(model, reward, next_state, done, discount_factor)\n",
    "    \n",
    "    # loss is measured from error between current and newly expected Q values\n",
    "    loss = F.smooth_l1_loss(q_val, target)\n",
    "\n",
    "    # backpropagation of loss to t Network (PyTorch magic)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()  # Returns a Python scalar, and releases history (similar to .detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episodes(train, model, memory, env, num_episodes, batch_size, discount_factor, learn_rate, TargetComputer):\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), learn_rate)\n",
    "    \n",
    "    global_steps = 0  # Count the steps (do not reset at episode start, to compute epsilon)\n",
    "    episode_durations = []  #\n",
    "    losses = []\n",
    "    \n",
    "    \n",
    "    for i in tqdm(range(num_episodes)):\n",
    "        # YOUR CODE HERE\n",
    "        # following algo from here: https://drive.google.com/file/d/0BxXI_RttTZAhVUhpbDhiSUFFNjg/view\n",
    "        \n",
    "        episode_duration = 0\n",
    "        s = env.reset()\n",
    "        while True:\n",
    "            # a = select_action(model, s, -10); # print('Careful: running with all random action selection') # epsilon-greedy select action\n",
    "            a = select_action(model, s, get_epsilon(global_steps)) # epsilon-greedy select action\n",
    "            s_next, r, done, _ = env.step(a)  # execute action a_t in emulator\n",
    "            episode_duration += 1\n",
    "                \n",
    "            memory.push((s, a, r, s_next, done))  # store transition in D\n",
    "            global_steps += 1\n",
    "            s = s_next\n",
    "            if done:\n",
    "                break\n",
    "            loss = train(model, memory, optimizer, batch_size, discount_factor, TargetComputer)\n",
    "            losses.append(loss)\n",
    "        episode_durations.append(episode_duration)\n",
    "    plt.plot(losses[900:])\n",
    "    \n",
    "    return episode_durations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's run it!\n",
    "\n",
    "\n",
    "def get_state_number(env):\n",
    "    if isinstance(env.env.observation_space, gym.spaces.box.Box):\n",
    "        nS = env.env.observation_space.shape[0]\n",
    "    elif isinstance(env.env.observation_space, gym.spaces.discrete.Discrete):\n",
    "        nS = env.env.observation_space.n\n",
    "    else:\n",
    "        print(\"Encountered unknown class type in state: {}. Exiting code execution\".format(type(item[1])))\n",
    "        exit()    \n",
    "    return nS\n",
    "\n",
    "def get_action_number(env):\n",
    "    \n",
    "\n",
    "    if isinstance(env.env.action_space, gym.spaces.box.Box):\n",
    "        nA = env.env.action_space.shape[0]\n",
    "    elif isinstance(env.env.action_space, gym.spaces.discrete.Discrete):\n",
    "        nA = env.env.action_space.n\n",
    "    else:\n",
    "        print(\"Encountered unknown class type in action: {}. Exiting code execution\".format(type(item[1])))\n",
    "        exit()    \n",
    "        \n",
    "    env.env.observation_space\n",
    "    return nA\n",
    "\n",
    "def run_experiment(hyperparams):\n",
    "    \n",
    "    num_episodes = hyperparams['num_episodes']\n",
    "    batch_size = hyperparams['batch_size']\n",
    "    discount_factor = hyperparams['discount_factor']\n",
    "    learn_rate = hyperparams['learn_rate']\n",
    "    memory = hyperparams['memory']\n",
    "    num_hidden = hyperparams['num_hidden']\n",
    "    seed = hyperparams['seed']\n",
    "    target_network_steps = hyperparams['target_network_steps']\n",
    "    env = hyperparams['env']\n",
    "\n",
    "    # We will seed the algorithm (before initializing QNetwork!) for reproducability\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    env.seed(seed)\n",
    "    \n",
    "    nA = get_action_number(env)\n",
    "    nS = get_state_number(env)\n",
    "\n",
    "    model = QNetwork(num_hidden=num_hidden, num_a=nA, num_s=nS)\n",
    "\n",
    "    TargetComputer = targetComputer(target_network_steps=target_network_steps)\n",
    "\n",
    "    episode_durations = run_episodes(train, model, memory, env, num_episodes, batch_size, discount_factor, learn_rate, TargetComputer)\n",
    "\n",
    "    # And see the results\n",
    "    def smooth(x, N):\n",
    "        cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "        return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
    "    plt.figure()\n",
    "    plt.plot(smooth(episode_durations, 10))\n",
    "    plt.title('Episode durations per episode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 69/100 [00:08<00:05,  5.92it/s]"
     ]
    }
   ],
   "source": [
    "mountainEnv = gym.envs.make(\"MountainCar-v0\")\n",
    "pendulumEnv = gym.envs.make(\"Pendulum-v0\")\n",
    "acrobotEnv = gym.envs.make(\"Acrobot-v1\")\n",
    "cartPoleEnv = gym.envs.make(\"CartPole-v1\")\n",
    "mountainCarContEnv = gym.envs.make(\"MountainCarContinuous-v0\")\n",
    "\n",
    "envs = [mountainEnv, acrobotEnv, cartPoleEnv]\n",
    "#mountainEnv works\n",
    "#pendulumEnv doesn't work..\n",
    "#acrobotEnv works\n",
    "#cartPoelEnv works\n",
    "#mountainCarContEnv doesn't work..\n",
    "\n",
    "# both don't work because they have a continuous action space, I think.\n",
    "\n",
    "# currently, cartPoleEnv is nicest. It divergew with useTrick = False, and converges with useTrick=True\n",
    "\n",
    "\n",
    "hyperparams = {\n",
    "    'env':                  cartPoleEnv,  # the environment\n",
    "    'num_episodes':         100,         \n",
    "    'batch_size':           64,\n",
    "    'discount_factor':      0.8,\n",
    "    'learn_rate':           1e-3,\n",
    "    'memory':               ReplayMemory(10000, useTrick=True), # set useTrick=False for no memory replay\n",
    "    'num_hidden':           128,\n",
    "    'seed':                 42,  # This is not randomly chosen <-- from TAs\n",
    "    'target_network_steps': 0   # If set to 1, (I THINK) this means we update the target network every step, thus we actually do not use a target network\n",
    "}\n",
    "\n",
    "\n",
    "# We can e.g. do grid-search here.\n",
    "# for env in envs:\n",
    "#     hyperparams['env'] = env\n",
    "#     run_experiment(hyperparams)\n",
    "\n",
    "run_experiment(hyperparams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
